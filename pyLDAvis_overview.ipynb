{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import matplotlib\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis.gensim\n",
    "import sys\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_to_dict(dir_path):\n",
    "    \"\"\"\n",
    "    Read file in specified path\n",
    "    :param file_path: Path of file to read\n",
    "    :return: List of characters conversations\n",
    "    \"\"\"\n",
    "    conversations = {}\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.abspath(os.path.join(dir_path, filename))\n",
    "            with open(file_path, 'r') as f:\n",
    "                x = f.readlines()\n",
    "                content_unicode = unicode(x[0], encoding='utf-8', errors='replace')\n",
    "                if not filename in conversations:\n",
    "                    conversations[filename.split('.')[0]] = content_unicode\n",
    "    return conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc, stopwords, punctuation, lemma):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in punctuation)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_term_matrix(doc_clean):\n",
    "    # Creating the term dictionary of our corpus, where every unique term is assigned an index.\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    return dictionary, doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_topics(num_topics, passes, dictionary, doc_term_matrix, n_top_terms):\n",
    "    # Creating the object for LDA model using gensim library\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "    # Running and Trainign LDA model on the document term matrix.\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    topic_words = []\n",
    "\n",
    "    for i in range(num_topics):\n",
    "        tt = ldamodel.get_topic_terms(i, n_top_terms)\n",
    "        topic_words.append([dictionary[pair[0]] for pair in tt])\n",
    "\n",
    "    return ldamodel, topic_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"./Spongebob NLP\"\n",
    "conversations = read_files_to_dict(dir_path)\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "normalized_conversations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in conversations.values():\n",
    "    normalized = preprocess(doc=doc, stopwords=stop, punctuation=exclude, lemma=lemma)\n",
    "    normalized_conversations.append(normalized.split())\n",
    "\n",
    "    dictionary, doc_term_matrix = get_doc_term_matrix(doc_clean=normalized_conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel, topics = get_lda_topics(num_topics=3,\n",
    "                                  passes=50,\n",
    "                                  dictionary=dictionary,\n",
    "                                  doc_term_matrix=doc_term_matrix,\n",
    "                                  n_top_terms=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, u'0.013*\"im\" + 0.010*\"get\" + 0.008*\"spongebob\"'),\n (1, u'0.002*\"class\" + 0.001*\"school\" + 0.001*\"boating\"'),\n (2, u'0.025*\"spongebob\" + 0.010*\"patrick\" + 0.009*\"krabs\"')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=3, num_words=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.show(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "name": "pyLDAvis_overview.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
